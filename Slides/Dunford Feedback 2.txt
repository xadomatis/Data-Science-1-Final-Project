Dunford Feedback 2

Good job, especially setting the problem up and walking us through some visualizations that provide insight into your outcome. You rushed through the ML section w/r/t why you only chose tree-based models, how you tuned, etc. Just be sure to be really clear re: why you make the decisions that you do in your report. Regarding the prefect prediction, this is likely due to a single (or set of predictors) perfectly aligning with the state of an outcome (that is, when x1 = 1 and x2 = 1 then y always equals 1). This will actually lead to non-convergence for a linear model. The issue is that there is no variation for specific subsets of the data when you turn them into binary features, The solution is to figure out which variables are causing (e.g. when x1 = 1 and x2 = 1 then y always equals 1... you want this to be that there are some 0s for every potential subset of a binary categorization ... put differently, your outcome must vary across your feature set) and either (a) don't dichotomize and keep continuous or (b) drop those features. Note that think of this cleaning process not as a nuisance but an opportunity to learn something interesting about these data. All in all, great work! -.5 for being over the time limit. I'm looking forward to reading the final result!
Eric Dunford, Dec 6 at 9:48am 